{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## 1: Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# optional: update OS packages in Amazon SageMaker Studio Ubuntu environment\n",
    "sudo bash -c 'export DEBIAN_FRONTEND=noninteractive && apt-get update -qq -y && apt-get upgrade -qq -y'\n",
    "\n",
    "# install dependencies\n",
    "sudo bash -c 'export DEBIAN_FRONTEND=noninteractive && apt-get install -y git git-lfs libgl1 ffmpeg wget'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker boto3 botocore ffmpeg-python ipython diffusers pywget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel after installing new packages\n",
    "\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2: Prepare the SVD-XT Model for Inference\n",
    "\n",
    "Steps to prepare the model for inference: 1/ Download the model artifacts from Hugging Face, 2/ add the custom inference script, 3/ create an archive file from the model artifacts, and 4/ upload the archive file to Amazon S3 for deployment.\n",
    "\n",
    "Alternately, for steps 2.2-2.4, below, if the model archive is already available from Amazon S3, see '2.2-2.4: Alternate Method if Model Already Exists in S3', below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 2.1: Import Packages and Set SageMaker Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n",
    "from sagemaker.s3 import s3_path_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_session_bucket = None\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "\n",
    "if sm_session_bucket is None and sm_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sm_session_bucket = sm_session.default_bucket()\n",
    "try:\n",
    "    sm_role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam_client = boto3.client(\"iam\")\n",
    "    sm_role = iam_client.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of packaged model archive file\n",
    "MODEL_ARCHIVE = \"model_v2.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker role arn: {sm_role}\")\n",
    "print(f\"sagemaker bucket: {sm_session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sm_session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2.2: Download the Model Artifacts from Hugging Face\n",
    "\n",
    "It will take 6-7 minutes to download model artifacts from Hugging Face. You will need a Hugging Face account to get your personal access token. Requires approximately 34 GB of space.\n",
    "\n",
    "Check the `/dev/nvme1n1` volume, mounted to `/home/sagemaker-user` to ensure it has enough space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "df -h $PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Downloading the model weights from the HuggingFace repository requires a username and personalized access token.\n",
    "\n",
    "You can create a simple READ-only access token in your [HuggingFace profile settings](https://huggingface.co/settings/tokens).\n",
    "\n",
    "### 403 Access Denied errors\n",
    "\n",
    "If you encounter errors during cloning, you need to make sure your username and access token are correct, and that you have accepted the Terms & Conditions of the Stable Video Diffusion model. Visit the [model card](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1) and accept the terms to get access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sh\n",
    "\n",
    "user_name=\"<YOUR_HUGGINGFACE_USERNAME>\"\n",
    "access_token=\"<YOUR_HUGGING_FACE_ACCESS_TOKEN>\"\n",
    "\n",
    "git clone \"https://${user_name}:${access_token}@huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 2.4: TAR GZIP Model Artifacts\n",
    "\n",
    "Important: Final model archive file will be 14-15 GB and **takes 20-30 minutes** to package and compress.\n",
    "\n",
    "Continuously poll the size of the model archive file file every 15 seconds from your terminal:\n",
    "\n",
    "```sh\n",
    "while sleep 15; do ls -la model_v2.tar.gz; done\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import shutil\n",
    "\n",
    "model_repo = \"stable-video-diffusion-img2vid-xt-1-1/\"\n",
    "\n",
    "# copy custom inference script and requirements.txt to model repo\n",
    "shutil.copy(\"inference/inference.py\", model_repo)\n",
    "shutil.copy(\"inference/requirements.txt\", model_repo)\n",
    "\n",
    "# use CLI tools to create model archive (faster than Python-based tar'ing)\n",
    "! cd {model_repo} && tar --verbose --exclude='.[^/]*' -c --gzip --file ../{MODEL_ARCHIVE} ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 2.2-2.4: Alternate Method if Model Already Exists in S3\n",
    "\n",
    "If the model archive file already exists in S3, skip steps 1-3 above. Create an Amazon S3 presigned URL and use the URL to download the model package. This replaces the two steps above: downloading the model artifacts and TAR GZIP. This step takes 4-7 minutes in the same AWS Region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "from pywget import wget\n",
    "\n",
    "presigned_s3_url = \"<YOUR_PRESIGNED_URL_GOES_HERE>\"\n",
    "\n",
    "wget.download(presigned_s3_url, MODEL_ARCHIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 2.5: Copy Model Artifacts to S3\n",
    "\n",
    "This step takes 2-3 minutes in the same AWS Region to copy model archive file to Amazon S3, which is approximately 14 GB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "response = s3_client.upload_file(\n",
    "    MODEL_ARCHIVE,\n",
    "    sm_session_bucket,\n",
    "    f\"async_inference/model/{MODEL_ARCHIVE}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 3: Deploy Model to Amazon SageMaker Endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 3.1: Deploy Model to Amazon SageMaker Endpoint\n",
    "\n",
    "Deploying the Amazon SageMaker Asynchronous Inference Endpoint takes 5-7 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"3600\",\n",
    "    \"TS_MAX_RESPONSE_SIZE\": \"1000000000\",\n",
    "    \"TS_MAX_REQUEST_SIZE\": \"1000000000\",\n",
    "    \"MMS_MAX_RESPONSE_SIZE\": \"1000000000\",\n",
    "    \"MMS_MAX_REQUEST_SIZE\": \"1000000000\",\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_path_join(\n",
    "        \"s3://\", sm_session_bucket, f\"async_inference/model/{MODEL_ARCHIVE}\"\n",
    "    ),\n",
    "    transformers_version=\"4.37.0\",\n",
    "    pytorch_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    env=env,\n",
    "    role=sm_role,\n",
    ")\n",
    "\n",
    "# where the response payload or error will be stored\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=s3_path_join(\"s3://\", sm_session_bucket, \"async_inference/output\"),\n",
    "    failure_path=s3_path_join(\n",
    "        \"s3://\", sm_session_bucket, \"async_inference/output_errors\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.4xlarge\",\n",
    "    async_inference_config=async_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 3.2: Optional: Set Endpoint Name Manually\n",
    "\n",
    "If the model was previously deployed to an endpoint, then uncomment and set the `endpoint_name` variable manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name = \"<YOUR_MODEL_ENDPOINT_NAME>\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
