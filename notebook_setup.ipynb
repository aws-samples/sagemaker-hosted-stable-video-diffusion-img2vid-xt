{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook Requirements\n",
    "\n",
    "This notebook requires:\n",
    "* at least `ml.m5.2xlarge` instance\n",
    "* at least 80 GiB of storage\n",
    "\n",
    "Otherwise, the download and packaging of SVD model assets might fail. If the below mentioned storage sizes do not match your system, please verify the requirements are matched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1: Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# optional: update system packages in Amazon SageMaker Studio Ubuntu environment\n",
    "sudo bash -c 'export DEBIAN_FRONTEND=noninteractive && apt-get -qq update -y && apt-get -qq upgrade -y'\n",
    "\n",
    "# install system packages\n",
    "sudo bash -c 'export DEBIAN_FRONTEND=noninteractive && apt-get -qq install -y git git-lfs libgl1 ffmpeg wget pigz pv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# install new Python packages\n",
    "pip install -Uq sagemaker boto3 botocore ffmpeg-python ipython diffusers pywget opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart Python kernel after installing packages\n",
    "\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2: Prepare the SVD-XT Model for Inference\n",
    "\n",
    "Steps to prepare the model for inference: 1/ Download the model artifacts from Hugging Face, 2/ add the custom inference script, 3/ create an archive file from the model artifacts, and 4/ upload the archive file to Amazon S3 for deployment.\n",
    "\n",
    "Alternately, for steps 2.2-2.4, below, if the model archive is already available from Amazon S3, see **2.2-2.4: Alternate Method if Model Already Exists in S3'**, below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2.1: Import Packages and Set SageMaker Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n",
    "from sagemaker.s3 import s3_path_join\n",
    "\n",
    "\n",
    "MODEL_REPO_PATH = \"stable-video-diffusion-img2vid-xt-1-1/\"\n",
    "MODEL_ARCHIVE = \"model_v2.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_session_bucket = None\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "\n",
    "if sm_session_bucket is None and sm_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sm_session_bucket = sm_session.default_bucket()\n",
    "try:\n",
    "    sm_role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam_client = boto3.client(\"iam\")\n",
    "    sm_role = iam_client.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker role arn: {sm_role}\")\n",
    "print(f\"sagemaker bucket: {sm_session_bucket}\")\n",
    "print(f\"sagemaker session region: {sm_session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2.2: Download the Model Artifacts from Hugging Face\n",
    "\n",
    "It will take 6-7 minutes to download model artifacts from Hugging Face. You will need a Hugging Face account to get your personal access token. Requires approximately 34 GB of space.\n",
    "\n",
    "Check the `/dev/nvme1n1` volume, mounted to `/home/sagemaker-user` to ensure it has enough space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "df -h $PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Downloading the model weights from the HuggingFace repository requires a username and personalized access token.\n",
    "\n",
    "You can create a simple READ-only access token in your [HuggingFace profile settings](https://huggingface.co/settings/tokens).\n",
    "\n",
    "### 403 Access Denied errors\n",
    "\n",
    "If you encounter errors during cloning, you need to make sure your username and access token are correct, and that you have accepted the Terms & Conditions of the Stable Video Diffusion model. Visit the [model card](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1) and accept the terms to get access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "user_name = \"<YOUR_HUGGINGFACE_USERNAME>\"\n",
    "access_token = \"<YOUR_HUGGING_FACE_ACCESS_TOKEN>\"\n",
    "\n",
    "# use CLI tool to clone the repo with working credentials\n",
    "! git clone \"https://{user_name}:{access_token}@huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1.git\" {MODEL_REPO_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 2.4: Package the Model Artifacts\n",
    "\n",
    "The final model archive file will be **approx. 14 GiB** and takes **about 10 minutes on an ml.m5.2xlarge instance** to package and compress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import shutil\n",
    "\n",
    "# copy custom inference script and requirements.txt to model repo\n",
    "shutil.copy(\"inference/inference.py\", MODEL_REPO_PATH)\n",
    "shutil.copy(\"inference/requirements.txt\", MODEL_REPO_PATH)\n",
    "\n",
    "# use CLI tools to create model archive (faster than Python-based tar'ing)\n",
    "! cd {MODEL_REPO_PATH} && tar --verbose --use-compress-program=\"pigz --best --recursive\" --exclude='.[^/]*' -c . | pv --timer --bytes > ../{MODEL_ARCHIVE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 2.2-2.4: Alternate Method if Model Already Exists in S3\n",
    "\n",
    "If the model archive file already exists in S3, skip steps 1-3 above. Create an Amazon S3 presigned URL and use the URL to download the model package. This replaces the two steps above: downloading the model artifacts and TAR GZIP. This step takes 4-7 minutes in the same AWS Region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "from pywget import wget\n",
    "\n",
    "# presigned_s3_url = \"<YOUR_PRESIGNED_URL_GOES_HERE>\"\n",
    "# wget.download(presigned_s3_url, MODEL_ARCHIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 2.5: Upload Model Archive to S3\n",
    "\n",
    "This step takes 2-3 minutes in the same AWS Region to copy model archive file to Amazon S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "f = IntProgress(min=0, max=os.path.getsize(MODEL_ARCHIVE), description=\"Uploading:\")\n",
    "display(f)\n",
    "def progress_update(bytes_amount):\n",
    "    f.value += bytes_amount\n",
    "\n",
    "print(f\"Uploading model archive {MODEL_ARCHIVE} to S3 bucket {sm_session_bucket}...\")\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "response = s3_client.upload_file(\n",
    "    Filename=MODEL_ARCHIVE,\n",
    "    Bucket=sm_session_bucket,\n",
    "    Key=f\"async_inference/model/{MODEL_ARCHIVE}\",\n",
    "    Callback=progress_update,\n",
    ")\n",
    "print(response)\n",
    "print(\"Upload completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 3: Deploy Model to Amazon SageMaker Endpoint\n",
    "\n",
    "Deploying the Amazon SageMaker Asynchronous Inference Endpoint takes 5-7 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env = {\n",
    "    \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"3600\",\n",
    "    \"TS_MAX_RESPONSE_SIZE\": \"1000000000\",\n",
    "    \"TS_MAX_REQUEST_SIZE\": \"1000000000\",\n",
    "    \"MMS_MAX_RESPONSE_SIZE\": \"1000000000\",\n",
    "    \"MMS_MAX_REQUEST_SIZE\": \"1000000000\",\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_path_join(\n",
    "        \"s3://\", sm_session_bucket, f\"async_inference/model/{MODEL_ARCHIVE}\"\n",
    "    ),\n",
    "    transformers_version=\"4.37.0\",\n",
    "    pytorch_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    env=env,\n",
    "    role=sm_role,\n",
    ")\n",
    "\n",
    "# where the response payload or error will be stored\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=s3_path_join(\"s3://\", sm_session_bucket, \"async_inference/output\"),\n",
    "    failure_path=s3_path_join(\n",
    "        \"s3://\", sm_session_bucket, \"async_inference/output_errors\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.4xlarge\",\n",
    "    async_inference_config=async_config,\n",
    ")\n",
    "\n",
    "with open(\"deployed_endpoint_name.txt\", \"w\") as f:\n",
    "    f.write(predictor.endpoint_name)\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Deployed endpoint name: {predictor.endpoint_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
